<meta charset="utf-8" emacsmode="-*- markdown -*-"><link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/apidoc.css?">

# Deliverables: Wk 3


## Frequency Mapping

This builds on the beat detection from week 2. 
I used log-power spectrogram data to find the most dominant frequency within each time-segment corresponding to a beat.
These frequency bands are centered on the semi tones of the diatonic scale system, using A=440Hz. 
By binning into these frequency bands, the goal is to have a more perceptually-motivated picture of where the dominant frequency in the music is. 
The code to produce this binned-spectrogram was taken from the source below. 

One caveat I noticed is that in music with a lot of percussive elements (ex: claps, high hat, etc), there is a lot of high frequency power that don't necessarily correspond to melodic or harmonic cpmponents. 
To help aid in ignoring these, I cut off the very highest frequency components, above about a C7. 

I also played around with using a chromagram, which further condenses the frequency components into the 12 semi-tones in a single octave. 
However, this had a lot of repeated dominant frequencies even when the actual melodic and harmonic content varied, so I chose to stick with the log-power spectrogram.

The process was as follows:
- Compute the log-power spectrogram
- Find the maximum frequency band for each window
- Find the most frequent max frequency over all the windows within a beat's time-segment
- Map the resulting frequency bands to range over the whole hue spectrum [0,255] (FastLED uses this range for easier computation.)
Thus, the exact hue mapping depends on the range of the song, so that it can show the greatest possible color difference between adjacent notes in a song. 

![Figure [1]: FastLED hue spectrum](fastLED_hues.jpg)

Hue mappings for sample songs (using matplotlib 'hsv' color map, so not perfect correspondance to FastLED spectrum)


![Never Gonna Give You Up](demo_songs\nggyu_colors.png)
![Piano Man](demo_songs/PianoMan_colors.png)

Resynthesized frequencies: 
Note: because of the binning process, these sound a bit out of tune and I fully acknowledge that. However, I still think they do a decent job of capturing what is going on. 
Furthermore, when there are things like strong step-wise ascents and descents, they do usually reflect that, which should tranlate well to the LED patterns. 
WARNING: very high frequencies!! Please turn down your sound!! 


![Never Gonna Give You Up](demo_songs/nggyu_colors_superimposed.mp3)
![Stayin' Alive](demo_songs/stayin_alive_colors_superimposed.mp3)



Sources: https://www.audiolabs-erlangen.de/resources/MIR/FMP/C3/C3S1_SpecLogFreq-Chromagram.html



## Integrate Frequency Data into LED Patterns

The video below displays that, given data structures for the frequency values in a song, the LED can perform a light show with adaptive changes in hue:
![Video](https://www.youtube.com/watch?v=hWuwHcWFKDU)


I built onto the "light show" state machines from last week, so that they integrate the song's frequency data that is to be received from the server. 
In the pattern shown on the video, the LED lights "flash" together" to the beat with an appropriate color. (Another pattern, where two LED lights are updated at a time to create a "chasing" sequence of lights, is in the play_lightshow.ino code.)

As explained in the above section, the [0,256) Hue values of the HSV scale linearly map to ROYGBIV, so using the frequency-to-hue mapping values passed from the Frequency Mapping process, we are able to integrate audio (frequency) signal inputs with visual (Color) signal outputs.

The state machine is structured as follows:
	- IDLE state: FSM begins here, as an initial point before immediately transitioning to the COLOR state.
	- COLOR state: determine the color to switch the LED light(s) to, by extracting the correct frequency value from the notes array. Switch to the RETAIN state.
	- RETAIN state: In the case of the "chase" pattern, first determine which LED lights to update. Retain the color on the LED light(s) for the appropriate duration of time as specified in the timestamp[] array, unless the end of the song has been reached. Switch to the COLOR state to repeat the LED update process.


## Server Side Database Structure

We created two SQL tables to store the data, one table stores per useri information (the user table) and the other table stores per room information (the room table).

The user table has the following fields:
- **User** - Unique identifier for arduino board that needs to be entered on the webapp
- **Room** - The room associated with this user, controlled by the web app
- **Need update song?** - if true, the server will send information abotu the song to the arduino in its next query
- **Need update play/pause state?** - if true, the server will send information abotu the play/pause state to the arduino in its next query

The room table has the following fields:
- **Room** - the room name
- **Song** - the name of the song being played in the room
- **Paused** - boolean flag of if the audio is currently paused or not
- **Position** - the timestamp/position of the song in seconds

The devserver maintains this data dependent on specific get/post requests from the server. See code below:

```
            if message[0] == 'user':
                user, room = message[1:]
                update_user_room(user, room)
                return get_room_attr(room, 'song')

            if message[0] == 'room':
                room, song = message[1:]
                set_room_attr(room, 'song', song)
                set_user_room_attr(room, 'song_changed', 1)
                return song

            if request['values']['reason'] == 'pause':
                set_room_attr(request['values']['room'], 'paused', 1)
                set_user_room_attr(request['values']['room'], 'pause_changed', 1)

            if request['values']['reason'] == 'play':
                set_room_attr(request['values']['room'], 'paused', 0)
                set_user_room_attr(request['values']['room'], 'pause_changed', 1)
                set_room_attr(request['values']['room'], 'position', request['values']['position'])

```

Then based on the priority of updating about the song first and then the play/pause state, when the esp32 periodicially queries the dev server, we send an update back!

For this week, we put pre-processed times for our sample songs onto the server as txt files. This is because it takes a couple seconds to process a song (~4s), and there is no good way to build this time in until we have a song queue (which is a week 4 goal).
When queried by the ESP, it returns a string of the times and colors corresponding to that song, if and only if the song has just been selected in the webapp. 

## Web App Requests

On the web app, we set up a periodic asynchronous request to share the data relating to the room and song and timestamp interfacing with the above dev server communication app.
<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

