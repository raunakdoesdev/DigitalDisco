<meta charset="utf-8" emacsmode="-*- markdown -*-"><link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/apidoc.css?">

# Deliverables: Wk 2


## Beat Detection

You can see this functionality here, with the original audio overlayed with the computer-generated beat track:

![Stayin' Alive](demo_songs/stayin_alive_superimposed.mp3)
![Never Gonna Give You Up](demo_songs/nggyu_superimposed.mp3)
![Stitches](demo_songs/stitches_superimposed.mp3)
![Piano Man](demo_songs/pianoman_superimposed.mp3)

The basic process for this beat detection algorithm is as follows: 
- compute the STFT of the song using scipy
- compute the spectral difference 
- find peaks in the spectral difference and return the corresponding times

The spectral difference is a function defined as follows: 
$$SD(n) = \sum_{k=0}^{N-1} \{H(|X_n[k]| - |X_{n-1}[k]|)\}^2$$
where: $H(x) = \frac{x+|x|}{2}$. 

This spectral difference function, as taught in 6.003, is a a measure of how much the frequency content is changing between frames. 
When there is a beat or a note change, we expect there to be a significant change in the frequency content. 
This is especially true for percussive attacks because they have frequency content in a very wide band of frequencies right at the onset of the note, which then drops off quickly. 

The function H(x) serves to focus on these onsets because it is positive when $X_n[k] >X_{n-1}[k]$ and 0 otherwise. Therefore, it selects for times where there is a sharp increase in the total amount spectral content. 

The peak finding algorithm works by iteratively selecting the maximum spectral difference sample above a certain threshold and then zeroing out its neighbors (to limit how close together the beats can fall). 
```python
def find_peaks(x, t, threshold, min_spacing):
    x = x.copy()
    output = []
    cand = max(x)
    i_find = {elt: i for i, elt in enumerate(x)}
    while cand > threshold:
        i = i_find[cand]
        output.append(t[i])
        for j in range(i-min_spacing+1, i+min_spacing):
            if 0 <= j < len(x):
                x[j] = 0
        cand = max(x)
    return sorted(output)
```
Those measures for minimum spacing and threshold have to be experimentally derived, but it appears that they can be tuned so that one set of values works across a wide range of songs. 
In this case, I designed that beats should not be any closer than about 250 bpm, and the threshold was determined from plots of the spectral difference. 

Finally, to create the demos, I synthesized a beat track using the provided times and an wav file of a drum hit (there is a drum hit at time 0 so that it will sync with the original song). 


## LEDs Control
A demo is here, where the LED is capable of switching between three colors:
![](https://youtu.be/SS_Rp8J9wRg)

This was implemented by using the FastLED library to send the appropriate data to the appropriate pin on the ESP32.

## Light Show with Integrated Time Data
A demo is here, where the LED changes color at the times in which musical notes change:
![](https://youtu.be/xGVIh3mCBFg)

To implement this, we essentially created a pattern (as in above LEDs Control section) where the notes switch time according to a given array of numbers.
These numbers represent times at which the music changes note. In this show, we manually created a toy array (see time changes printed on the Serial on the video):
~~~~~~~~~~~~~~~~~~
float times[] = {0.0 * 1000, 2.0 * 1000, 4.0 * 1000, 8.0 * 1000, 10.0 * 1000, 14.0 * 1000, 18.0 * 1000}; 
~~~~~~~~~~~~~~~~~~
In the context of the project, times[] would be note times from the current song playing as queried on the server.

Again, a light show relies on a state machine (instead of blocking code). In this specific light show, at each musical note the LED changes color, from green to blue. Of course there can be variations of this light show, where the colors change every other note, or the duration/rhythm of the notes dictate the next color.
We implemented the state machine therefore with a state for BLUE, state for GREEN, and a state for RETAIN (retain LED color for duration of note without switching). 


## Integrated Demo 

Server-side:

For this week, we put pre-processed times for our sample songs onto the server as txt files. 
This was mostly because when I put the signal processing code on the server, it through errors at me that numpy was trying to use too much memory. Something to deal with next <week class=""></week> 
When queried by the ESP, it returns a string of the times corresponding to that song, if and only if the song has just been selected in the webapp. 
There is no support for pausing yet. It's just based on the user hitting submit. 




<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>