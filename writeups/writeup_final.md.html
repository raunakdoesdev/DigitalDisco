<meta charset="utf-8" emacsmode="-*- markdown -*-"><link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/apidoc.css?">

**Digital Disco**
*Team 0: Sun Mee Choi, Raunak Chowdhuri, Karen Chung, Alyssa Solomon* 


The pandemic has put a pause on parties. Zoom parties can fill part of this need, but are lacking in atmosphere and experience. At a real party, you would blast pumped up music in a matching energetic environment, but this isn’t the case online. We aim to solve this pressing problem with our project, “Digital Disco.”

The Digital Disco is a smart lighting system that you can easily install in your room to take your virtual parties to the next level. Paired with our handy web application, your lights will auto-magically sync to the music you play on your computer speakers, which in turn will be synced with the music your friends play. 

The lightshow will sync to the beat of the music, with LED patterns which also depend on the frequencies in the song and the user's color preferences, chosen in the webapp. The ESP32 will consistently query a web server to gather information on the current lightshow and song status. 



Here is a demo of the full project: 

![Digital Disco Final Demo](https://youtu.be/3l9HZrM4_F0)


## Documentation: 
### Non-kit parts: 
Our only additional part was a length of individually addressable LEDs, which are compatible with the convenient FastLED library. 
This makes it relatively simple to design interesting patterns which are communicated over I2C. 
| Part Name/Description | Price | Item Number | Vendor URL |
|-------|-------|-------|-----|
| Arduino Controllable LED Strip | $10.88 | BTF-LIGHTING WS2812B RGB 5050SMD Individual Addressable 3.3FT 60(2x30)Pixels/m | [Amazon.com: BTF-LIGHTING WS2812B RGB 5050SMD Individual Addressable 16.4FT 60Pixels/m 300Pixels Flexible Black PCB Full Color LED Pixel Strip Dream Color IP30 Non-Waterproof Making LED Screen LED Wall Only DC5V: Home Improvement](https://www.amazon.com/dp/B01CDTED80/ref=as_li_ss_tl?pd_rd_i=B01CDTEJBG&pd_rd_w=PdPV2&pf_rd_p=45a72588-80f7-4414-9851-786f6c16d42b&pd_rd_wg=GEH20&pf_rd_r=BHD9QXXGHPXF3KXP5P0R&pd_rd_r=5730dbe5-46bd-4b1e-8a47-9450369807be&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUFKT0s1UVpGRURGM1kmZW5jcnlwdGVkSWQ9QTA2MzYxNjAxVkdYMDFTUVVOVjM5JmVuY3J5cHRlZEFkSWQ9QTAxMzAyMTYxVlAzQjBYR1FZTVJDJndpZGdldE5hbWU9c3BfZGV0YWlsJmFjdGlvbj1jbGlja1JlZGlyZWN0JmRvTm90TG9nQ2xpY2s9dHJ1ZQ&linkCode=sl1&tag=howto045-20&linkId=82a9d1c9949062ec0c731ab510fb3299&language=en_US&th=1) |

### System Diagrams: 
![System Diagram](final_system_diagram.PNG)

Above is the state diagram for our overall project. Note the communication between the major components: the dev server (with the databases), the webapp, the ESP32, and the LED. 
The "Functionality and Code Walk-through" section of this writeup will detail such operations, but as a high-level overview, the dev server stores information about the current users and rooms active in the Digital Disco, 
which is updated from the webapp via POST requests. The dev server then communicates with the ESP32 (which regularly queries the server) to send it the relevant information needed to process synchronous, colorful, and fun patterns onto the LED!
 
![Integrated ESP32 and LED State Machines](final_esp32_LEDs_fsm.PNG)
Zooming in from the overall system diagram, above is one state machine diagram that connects the state machine diagram for the ESP32 and the state machine diagram for the LEDs. Again, details on the states and functions of each respective state machine are found in the following sections of this writeup, 
but this diagram illustrates the high-level modes of the ESP32 and the LEDs, and how the former informs the activation of the latter.

![Wiring diagram](wiring_schematic.PNG)

Above is a diagram of our simple wiring setup. The pins on the LED strip are connected to the appropriate pins on the ESP32, with IO pin 19 being used as the data pin.

## Functionality and Code Walk-through: 

### Central Server Documentation: `comm.py, queue.db, pop_memory.txt`

The majority of the code in the central server consists of helper functions designed to streamline accessing the two main databases. Therefore, first we will outline the schema of the two SQL databases with an example entry in each:

**Rooms Database**

| Room (text)  | Song (text)                   | Paused (int) | Position (text) | Time (timestamp)                   |
| ------ | ----------------------- | ------ | -------- | ----------------------- |
| shared | Never Gonna Give You Up | 0      | 14.36    | datetime.datetime.now() |

We’ve defined the following helper functions to interact with this database:

- **create_room_table** - initialize the room database if it doesn’t exist
- **get_room_attr** - get a specific field from the active (most recent) row corresponding to the provided row - basically a convenient getter method for the rooms database
- **set_room_attr** - set the value of the column in the room table for a specific room
- **pop_song** - remove the most recent row in the rooms table for a specific room (corresponds to the currently active song on the queue for a room)
- **add_to_queue** - add a new row the rooms database with the given room and song
- **reset_queue** - remove all rows corresponding with a specific room id 

**User Database**

| User (text)   | Room (text)   | Pause Changed (int) | Song Changed (int) |
| ------- | ------ | ------------- | ------------ |
| raunakc | shared | 1             | 1            |

- **set_user_room_attr** - set the column value for all users who are in a specific room
- **get_user_attr** - get a column value for a specific user id
- **set_user_attr** - set the column value for a specific user id



Now, with the helper functions out of the way, we can get into the main functionality of the central server. First, let’s discuss the post request handling:

- To simplify things we sent all of the post request information in a single string called message delimited by the “|” character. Then we had three different possible post types. One was a requested queue reset, in which case we would reset the queue in a specific room, set the state for the room to be paused, update the “song_changed” as there should be no song playing as well as the “pause_changed” for all users in the room.

- Otherwise the request could be to add a song to the queue, in which case we would add a song to the queue, appending the led mode and times_and_freqs to the song name in our databse delimited by the `*` character.

- Finally, if the request was user specific, generally registering a specific user to a specific room, then we added the user to the specific room and set the user attribute of song changed and pause changed to needing an update so that the esp32 would receive an update immediately. 



In terms of get requests, there were two high level types:

- Server side (javascript specifially) get requests, which would specify play/pause/song query/or song end actions.
  - The javascript query just returns the song, the pause updates the pause variable for all of the users in the room, the play does the same but undoes the paused change.
- Esp32 queries - this queries for the server state. We have established a coding system for different types of messages where the first number in the message encodes some aspect of the state update.

### Web App Documentation: `app.py, media.html`

We used the Streamlit library to streamline the development of the webapp. This tool made it possible to do much of the front end work in Python using function calls like: `username = st.text_input(label='User Name')` to get the username from the user on the website.

Unfortunately, due to the need for realtime audio syncing–a feature which streamlit does not natively support yet–we could not rely solely on the streamlit infrastructure for making all of the web app.

For this reason, we include an HTML/javascript portion of the app as well, which is loaded up in the python script, modified, and then added dynamically to the webpage with the following code:

```python
st.write('### Media Player:')
with open('webapp/media.html', 'r') as f:
    audio = components.html(f.read().replace('MOTION_NAME_HERE', room)
                            .replace('ROOMGOESHERE', room),
                            height=50)
```

The motion and room name is automatically replaced based on values entered by the user so that the html/javascript portion of the app syncs with the correct room at all times.



The HTML Portion of the `media.html` file was fairly simple:

```html
<p id='buttons'>
    <button id='tostart'>Reset</button>
    <button id='pause'>Pause</button>
    <button id='forward'>Play</button>
    <button id='skip'>Skip 30 Ahead</button>
</p>
<audio controls id="player1" style="width:100%" autoplay>
    <source id="audiosource" src="SONGPATHHERE" type="audio/mpeg"/>
</audio>
```

It’s just four buttons and an audio player which is hidden (height = 0). The intelligence comes from the Javascript code which uses the Webtiming Sync library from https://webtiming.github.io/timingsrc/lib/timingsrc-v2.js to automatically sync the position in the audio.



We handle the button presses as follows:

- **Pause:** Update the song velocity to 0 and send a request to the server with the room id sharing the fact that the song was paused so that the ESP32 can receive that information and pause itself.
- **Reset:** Update the song position and velocity to 0 (stop and go to the beginning of the song) and send a pause request to the dev server so that it can tell the ESP32 to stop in its tracks!
- **Play:** Update the velocity to 1 (play at normal speed) and send a request to the dev server with the current position in the song and a notice that it has started playing.
- **Skip:** Skip forward 60 seconds in the position of the song and send a new play request to the server with the new position within the song.

Next, we have a series of requests that is scheduled to be sent out every 250 milliseconds.- 

- Basically, it queries the server to see what song is supposed to be playing in the current room. If the song is different from the song that is currently assigned to the audio source, then the audio src is replaced and the audio is reset to position and velocity 0. Additionally, if we find that the song duration exceeds the duration of the audio, then we send an audio end request to the server.

Finally—and this is on the Streamlit end again—we have the client side code for managing the queue. In general it is pretty simple, as the devserver manages the queue, but when the user presses the add to queue button, then we extract the colors and beats from the specific audio.


### Music Processing Documentation: `audio_processing.py`, song files
The use of the Streamlit app was also essential to being able to process audio in real-time, because it can run in the background while the music is playing without clogging up the server or running into the timeouts of an HTTP request. 

The main relevant function from this module is `colors_and_beats()`, which takes in a wav file, and returns a string containing a formatted list of all the times associated with beats, and a frequency-color mapped integer for each window of time. 
To find this information, we used two main techniques from signal processing: note onset detection, to find the beats within the song, and separate frequency analysis to find the dominant frequencies within these time periods. 

#### Beat Detection
The basic process for this beat detection algorithm is as follows: 
- compute the STFT of the song using scipy
- compute the spectral difference 
- find peaks in the spectral difference and return the corresponding times

The spectral difference is a function defined as follows: 
$$SD(n) = \sum_{k=0}^{N-1} \{H(|X_n[k]| - |X_{n-1}[k]|)\}^2$$
where: $H(x) = \frac{x+|x|}{2}$. 

This spectral difference function, as taught in 6.003, is a a measure of how much the frequency content is changing between frames. 
When there is a beat or a note change, we expect there to be a significant change in the frequency content. 
This is especially true for percussive attacks because they have frequency content in a very wide band of frequencies right at the onset of the note, which then drops off quickly. 

The function H(x) serves to focus on these onsets because it is positive when $X_n[k] >X_{n-1}[k]$ and 0 otherwise. Therefore, it selects for times where there is a sharp increase in the total amount spectral content. 

The peak finding algorithm works by iteratively selecting the maximum spectral difference sample above a certain threshold and then zeroing out its neighbors (to limit how close together the beats can fall). This process is repeated until no samples remain amove the threshold. 
Those measures for minimum spacing and threshold have to be experimentally derived, but it appears that they can be tuned so that one set of values works across a wide range of songs. 
In this case, we designed that beats should not be any closer than about 250 bpm, and the threshold was determined from plots of the spectral difference.

#### Frequency Analysis 
We used log-power spectrogram data to find the most dominant frequency within each time-segment corresponding to a beat.
These frequency bands are centered on the semi tones of the diatonic scale system, using A=440Hz. 
By binning into these frequency bands, the goal is to have a more perceptually-motivated picture of where the dominant frequency in the music is. 
The code to produce this binned-spectrogram was taken from the source below. 

One caveat we noticed is that in music with a lot of percussive elements (ex: claps, high hat, etc), there is a lot of high frequency power that don't necessarily correspond to melodic or harmonic cpmponents. 
To help aid in ignoring these, we cut off the very highest frequency components, above about a C7. 

We also played around with using a chromagram, which further condenses the frequency components into the 12 semi-tones in a single octave. 
However, this had a lot of repeated dominant frequencies even when the actual melodic and harmonic content varied, so we chose to stick with the log-power spectrogram.

The process was as follows:
- Compute the log-power spectrogram
- Find the maximum frequency band for each window
- Find the most frequent max frequency over all the windows within a beat's time-segment
- Map the resulting frequency bands to range over the whole hue spectrum [0,255] (FastLED uses this range for easier computation.)
Thus, the exact hue mapping depends on the range of the song, so that it can show the greatest possible color difference between adjacent notes in a song. 

Because of the binning process, the resynthesized frequencies sounded a bit out of tune and we fully acknowledge that. However, we still think they do a decent job of capturing what is going on. 
Furthermore, when there are things like strong step-wise ascents and descents, they do usually reflect that, which should tranlate well to the LED patterns. 

Sources: https://www.audiolabs-erlangen.de/resources/MIR/FMP/C3/C3S1_SpecLogFreq-Chromagram.html

Originally, we also wrote an audio-detection algorithm which would be used to help sync the ESP32 with the audio when it started playing. However, we ended up figuring out that this was unnecessary. 
This is in part because we realized we could successfully query the server very frequently from multiple devices, every 250 ms from the ESP32s as well as the webapp, which we were initially unsure of. 
It was much more reliable to query like this, and accept the slight latency, than use the audio detection. 

### Music-Synced LED Patterns Documentation: `lightshow.ino`
With the webapp, server-side communication, and signal processing working smoothly together, the LED's are the final visual piece to our Digital Disco party.  
The LEDs, based on the information provided from the ESP32, play a light pattern that is both synced with the music and customizable on the webapp.

#### ESP32 State Machine Implementation
Overall, the ESP32 state machine is how the system deals with start, pause, or play commands from the periodic GET requests to the server.

The state machine is structured as follows: 
* **IDLE**: nothing is being played on the web app, no LED sequences are running on the strip connected to the ESP32.
* **PLAYING**: the song is being played on the web app along with the corresponding LED sequence on the strip.
* **PAUSED**: the song has been paused on the web app, causing the LED sequence to stop in place.

When a new song is started on the web app, the GET request sends a numerically encoded message of START. The ESP32 also receives a string with comma-separated timestamps and frequencies, which are parsed into double arrays using `strtok`. These arrays do not get replaced unless there is a song change.

When the song is paused, the GET request sends a numerically encoded message of PAUSE and the sequence of color changes on the LED strip are paused.

When a song is played again after a pause, the GET request sends a numerically encoded message of PLAY along with the timestamp at which the song is resumed. Then a function is invoked that will adjust the timestamp array with the given resuming timestamp as the new zero by subtracting from all values in the original timestamp array. The new array is used to control the LED strip afterwards. Provided that the song is not over and there is no pause, the LED strip will continue to flash to the music.

Any skips are treated as resuming the song in a new place, and the timestamp array is adjusted as described previously.

Once a song is over, the ESP returns to the IDLE state until a new song is started.

No state machine is used in the process of sending GET requests themselves. This is controlled by one timer that causes a GET request to be sent every 250 milliseconds.

#### LED State Machine Implementation
Fundamentally, we implemented a basic FSM framework that creates an lightshow with adaptive changes in hue and value, given the current song's frequency and beat data. 

For the manipulation of LED's, we used the FastLED library compatible with our ESP32. The FastLED library supports LED control with the HSV (Hue, Saturation, Value) scale.
The [0,256) Hue values of the HSV scale linearly map to ROYGBIV; using the frequency-to-hue mapping values passed from the project's Frequency Mapping process, we were able to linearly match the audio (frequency) signal inputs with visual (Color) signal outputs. 
Of course, there was room for creativity in which audio frequencies correspond to what ranges of colors; we could use the classic ROYGBIV scale, but also the FastLED library's Palette function to generate more unusual gradients. 
As for integrating beats, our general approach was to change colors with every change of note (i.e. one "beat" change) in the song; however, there was additional flexibility in how many lights to change, where to change them, etc.

The state machine is structured as follows:
	- **IDLE** state: FSM begins here, as an initial point before immediately transitioning to the COLOR state.
	- **COLOR** state: determine the color to switch the LED light(s) to, by extracting the correct frequency value from the notes array. Switch to the RETAIN state.
	- **RETAIN** state: In the case of the "chase" pattern, first determine which LED lights to update. Retain the color on the LED light(s) for the appropriate duration of time as specified in the timestamp[] array, unless the end of the song has been reached. Switch to the COLOR state to repeat the LED update process.

#### LED Patterns Design
From this FSM backbone, we developed four unique types of patterns that would adapt to the current song's beats AND/OR frequency.
Below are details on the design decisions involved in recreating each pattern, and some corresponding key code components.

* **WAVES**: This pattern sends "waves" of colors darting across the LED strip at constant speed; the effect is achievable by commanding certain LED lights (indices returned from a sine-wave function) to turn on, and then fade at a certain rate.
		The colors shown correspond to the real-time frequency of the current note in the song. To clarify the colors and for better visual effect, we implemented the pattern to send two distinct waves (of the same color) with contrasting offsets, across a white background of LEDs. 
		This pattern does not correlate with the beat changes of the song; if a wave is sent at every note change (or even at every few note changes), the lights would be darting too quickly back and forth across the LED for it to be visually pleasing.
		Instead, the user would be able to specify if they want a "Fast Wave" or a "Slow Wave" pattern.
		The critical function to designing this pattern was FastLED's beatsin8, which generates numbers along a sinusoidal graph; these numbers correspond to the indices of the LEDs to turn on.
		Below shows the generation of three waves in the "Slow Waves" pattern, each at speed 2 and with ranges from the beginning and end of the LED strip, with three different offsets:
		~~~~~~~~~~~~~~~~~~~~~~~~~~
		if (speedy == 0) { // slow
			sinBeat = beatsin8(2, 0, NUM_LEDS - 1, 0, 0);
			sinBeat2 = beatsin8(2, 0, NUM_LEDS - 1, 0, 128);
			sinBeat3 = beatsin8(2, 0, NUM_LEDS - 1, 0, 255);
		}
		~~~~~~~~~~~~~~~~~~~~~~~~~~
		
* **GRADIENTS**: This pattern is similar to the Wave in that it sends a moving window of colors across the LED strip; but here, the window moves with a speed proportional to the song's speed.  
		This effect is achievable by manipulating the a "Palette" functionality in FastLED. We specified a palette of colors to feed into the FSM, and upon every switch of notes, the LED would display a shifted subset of the palette; over time, this would appear as the gradient "moving" across the LED.
		Since this pattern does not integrate the frequency data, We similarly gave the user the option to see a gradient of warmer colors ("Sun Gradient") or cooler colors ("Moon Gradient").
		The critical functions to designing this pattern were fill_palette() and DEFINE_GRADIENT_PALETTE(). I defined a gradient palette for each gradient palette, and used fill_palette() to sample from that color scheme with the change of beat. Here is an example, with the Sun Gradient:
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		DEFINE_GRADIENT_PALETTE( heatmap_gp ) {
		  0,     0,  0,  0,   //black
		  224,   255, 255,  0,  //bright yellow
		  128,   255,  0,  0,   //red
		  255,   255, 255, 255
		}; //full white
		....
		CRGBPalette16 palette = heatmap_gp;
		fill_palette(leds, num_leds, palette_idx, 255 / num_leds, palette, 255, LINEARBLEND); // palette_idx++ in the RETAIN state of FSM
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **SPARKLES**: This pattern uses both beat and frequency data. Upon each change of beat, it turns on a randomly selected LED light with a color corresponding to the current note's frequency. To achieve a continuous visual effect, We faded each turned-on LED by constant amounts at each loop, so that the LED strip as a whole has a natural sparkly look.
		The critical functions to designing this pattern were the standard random generator and fadeToBlackBy(); by switching a random LED light on, a cool sparkle would be generated, and it would fade like natural light.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		leds[random8(0, NUM_LEDS - 1)] = CHSV(freq, 250, 127);
		fadeToBlackBy(leds, NUM_LEDS, 1); 		// Fade all LEDs down by 1 in brightness each time this is called
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* **PULSES**: This pattern also uses both beat and frequency data.  Upon each change of beat, it turns on all LED lights with a color corresponding to the current note's frequency. 
		To achieve a continuous visual effect (i.e. make it look less like a series of discrete flashes), We used the "blend" functionality on the FastLED library, where a new color is not immediately "flashed" in replacement of the previous one, but rather blended with the preexisting color.
		Between the timespan of two notes with different frequencies, the LEDs would cycle within a blended range of the two notes' corresponding colors, creating a smooth and continuous look. 
		The critical functions to designing this pattern were FastLED's blend(), and my helper function cycle() with fill_gradient_RGB() called at every call of the FSM.
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
		endclr = blend(CHSV(last_freq, 250, 127), CHSV(freq, 250, 127), speed);  // end color
		midclr = blend(CHSV(freq, 250, 127), CHSV(last_freq, 250, 127), speed);  // middle color
		...
		cycle(); // call during RETAIN FSM state
		...
		void cycle() { // alternate between the end and middle color to create a smoothly transitioning/alternating pulse.
		  fill_gradient_RGB(leds, 0, endclr, NUM_LEDS / 2, midclr);  
		  fill_gradient_RGB(leds, NUM_LEDS / 2 + 1, midclr, NUM_LEDS, endclr);
		}
		~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	







## Appendix
All weekly demos, organized by topic:  

### Integrated examples:
![Server + ESP + lights + webapp demo (wk 3--sketchy)](https://youtu.be/aiEZ53-an0Y)
![Multiple users (wk 3)](https://youtu.be/GxD8v5DtT-w)
![Full demo (wk 4)](https://youtu.be/ym5JPS9DFGw)





### ESP and LEDs:
![Basic LED control (wk 2)](https://youtu.be/xGVIh3mCBFg)
![Light Show with Integrated Time Data (wk 2)](https://youtu.be/xGVIh3mCBFg)
![Integrate Frequency Data into LED Patterns](https://www.youtube.com/watch?v=hWuwHcWFKDU)

### Webapp and server comms:
![Three-way sync (wk 1)](https://www.youtube.com/watch?v=sueh-2Q8Fh8)
![Media-synced web app rooms (wk 1)](https://www.youtube.com/watch?v=aS6W9owh_R8)
![Changing rooms (wk 4)](https://youtu.be/YicCbGzw08g)
![Removing from queue (wk 4)](https://www.youtube.com/watch?v=czAikrdrauI)

### Signal processing:
##### Beat Detection (wk 2): 
![Stayin' Alive](demo_songs/stayin_alive_superimposed.mp3)
![Never Gonna Give You Up](demo_songs/nggyu_superimposed.mp3)
![Stitches](demo_songs/stitches_superimposed.mp3)
![Piano Man](demo_songs/pianoman_superimposed.mp3)
##### Frequency analysis (wk 3): 
WARNING: very high frequencies!! Please turn down your sound!! 
![Never Gonna Give You Up](demo_songs/nggyu_colors_superimposed.mp3)
![Stayin' Alive](demo_songs/stayin_alive_colors_superimposed.mp3)
![Never Gonna Give You Up](demo_songs/nggyu_colors.png)
![Piano Man](demo_songs/PianoMan_colors.png)


### Ultimately unused: 
![Audio detection (wk 1)](https://youtu.be/nwY2ylStbko)

<style class="fallback">body{visibility:hidden}</style><script>markdeepOptions={tocStyle:'medium'};</script>
<!-- Markdeep: --><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

First, the user enters the webapp, and inputs their username. This is used to link them uniquely to their individual ESP32 device. 
They select one of the available rooms from a drop down list, to accommodate listening by themselves, or with different subset groups of their friends at any given time. 
Then, they pick a song from the list, pick an LED pattern, and add it to the queue. The various LED patterns are based on the frequency of the song's notes, and the user's color preferences. 
The song is analyzed in real time to produce a lightshow that syncs to the beat of the music. 
The ESP32 consistently queries a web server to gather information pertaining to the lighting sequence it should play when the music starts, and the play/pause status of the music at any given moment. 

The user can also choose from different rooms, 